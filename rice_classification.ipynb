{"cells": [{"cell_type": "markdown", "id": "4d4e6e70-6f68-4470-aa0c-dcaf89090c24", "metadata": {}, "source": "# MACHINE LEARNING EN SPARK"}, {"cell_type": "markdown", "id": "ff8405d5-ef2f-40a9-97c4-8bd6aa7d4932", "metadata": {}, "source": "## Rice MSC Dataset"}, {"cell_type": "markdown", "id": "40ecaf09-cacc-4013-9195-08bbd43a70aa", "metadata": {}, "source": "### OBJETIVO\nLa finalidad del proyecto es lograr un modelo de predicci\u00f3n capaz de clasificar cada instancia referente a un grano de arroz en los cinco tipos de variedad de arroz observados:\n\n* Arborio\n* Basmati\n* Ipsala\n* Jasmine\n* Karacadag"}, {"cell_type": "markdown", "id": "f9391a67-6f32-4035-ad98-5b0c6dc7f02c", "metadata": {}, "source": "### INTRODUCCI\u00d3N\nSe realizaron procesos de extracci\u00f3n de caracter\u00edsticas basados en las t\u00e9cnicas de tratamiento de im\u00e1genes utilizando caracter\u00edsticas morfol\u00f3gicas, de forma y de color para cinco variedades diferentes de arroz de la misma marca. Se obtuvo un total de 75 mil piezas de granos de arroz, incluidas 15 mil piezas de cada variedad de arroz. Se aplicaron operaciones de preprocesamiento a las im\u00e1genes y se dispusieron para la extracci\u00f3n de caracter\u00edsticas. Se dedujeron un total de 106 caracter\u00edsticas de las im\u00e1genes; 12 caracter\u00edsticas morfol\u00f3gicas y 4 caracter\u00edsticas de forma obtenidas mediante caracter\u00edsticas morfol\u00f3gicas y 90 caracter\u00edsticas de color obtenidas a partir de cinco espacios de color diferentes (RGB, HSV, Lab*, YCbCr, XYZ). Adem\u00e1s, para las 106 caracter\u00edsticas obtenidas, se seleccionaron caracter\u00edsticas mediante pruebas ANOVA, X2 y Gain Ratio y se determinaron las caracter\u00edsticas \u00fatiles. En todas las pruebas, de los 106 rasgos, se obtuvieron los 5 rasgos m\u00e1s eficaces y espec\u00edficos redondez, compacidad, factor de forma 3, relaci\u00f3n de aspecto y excentricidad. Los rasgos de color se enumeraron en distinto orden siguiendo estos rasgos."}, {"cell_type": "markdown", "id": "4d21d42b-e417-4d65-8380-98b9f3620fd2", "metadata": {}, "source": "### AN\u00c1LISIS EXPLORATORIO DE DATOS"}, {"cell_type": "markdown", "id": "4026b248-8917-4813-96f4-bd8efa73afc3", "metadata": {}, "source": "#### Cargamos los datos"}, {"cell_type": "code", "execution_count": 83, "id": "dae3fdea-7bc0-4491-9dfb-214bb2a87db6", "metadata": {}, "outputs": [], "source": "df = spark.read.load(\"gs://bucket_fms/datos_proyecto/rice-msc-dataset.csv\",format = \"csv\", header = True)"}, {"cell_type": "markdown", "id": "994cc7bb-fbcc-4996-8a1b-3c9ca535e9ba", "metadata": {}, "source": "#### Vemos cuantas filas tiene el DataFrame"}, {"cell_type": "code", "execution_count": 84, "id": "cf99a3e0-53ea-4576-82f4-be6f27ef7ced", "metadata": {}, "outputs": [{"data": {"text/plain": "75000"}, "execution_count": 84, "metadata": {}, "output_type": "execute_result"}], "source": "df.count()"}, {"cell_type": "markdown", "id": "dafa2623-bf9b-47ca-a03c-dfa6509aa04e", "metadata": {}, "source": "#### Eliminamos las filas con valores faltantes"}, {"cell_type": "code", "execution_count": 85, "id": "915fd9e9-2134-4dd1-a81f-0b0648b695df", "metadata": {}, "outputs": [], "source": "df = df.dropna()"}, {"cell_type": "markdown", "id": "1bb5a806-f983-4085-a2cf-b6c7fc813339", "metadata": {}, "source": "#### Vemos ahora con cuantas filas nos hemos quedado"}, {"cell_type": "code", "execution_count": 86, "id": "adfd29a2-9b29-424b-8eb0-5cafa68c90a1", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "74992"}, "execution_count": 86, "metadata": {}, "output_type": "execute_result"}], "source": "df.count()"}, {"cell_type": "markdown", "id": "040155bb-eeba-4226-93a2-6ac27a2e6ffd", "metadata": {}, "source": "Hay pocas filas con valores nulos, as\u00ed que podemos asumir la p\u00e9rdida de los datos. Si tuvieramos muchas tendr\u00edamos que sustituir esos valores faltantes por la media/mediana de la variable u otro tipo de t\u00e9cnicas utilizadas sobre valores faltantes."}, {"cell_type": "markdown", "id": "04254230-6e71-4ecb-8cf6-3d1f53bbab7e", "metadata": {}, "source": "#### Vemos cuantas variables contienen los datos"}, {"cell_type": "code", "execution_count": 87, "id": "472f7f6d-ee9f-4b92-86c8-8bb8ac418d45", "metadata": {}, "outputs": [{"data": {"text/plain": "107"}, "execution_count": 87, "metadata": {}, "output_type": "execute_result"}], "source": "len(df.columns) # variables"}, {"cell_type": "markdown", "id": "a6adaaa1-673b-4012-aa61-613832d29666", "metadata": {}, "source": "#### El siguiente paso ser\u00e1 transformar el tipo de datos de cada variable a su tipo de datos correcto con la ayuda de los metadatos"}, {"cell_type": "code", "execution_count": 88, "id": "85c5e2a7-5749-4223-8e9b-bd23e618bfc8", "metadata": {}, "outputs": [{"data": {"text/plain": "[('AREA', 'string'),\n ('PERIMETER', 'string'),\n ('MAJOR_AXIS', 'string'),\n ('MINOR_AXIS', 'string'),\n ('ECCENTRICITY', 'string'),\n ('EQDIASQ', 'string'),\n ('SOLIDITY', 'string'),\n ('CONVEX_AREA', 'string'),\n ('EXTENT', 'string'),\n ('ASPECT_RATIO', 'string'),\n ('ROUNDNESS', 'string'),\n ('COMPACTNESS', 'string'),\n ('SHAPEFACTOR_1', 'string'),\n ('SHAPEFACTOR_2', 'string'),\n ('SHAPEFACTOR_3', 'string'),\n ('SHAPEFACTOR_4', 'string'),\n ('meanRR', 'string'),\n ('meanRG', 'string'),\n ('meanRB', 'string'),\n ('StdDevRR', 'string'),\n ('StdDevRG', 'string'),\n ('StdDevRB', 'string'),\n ('skewRR', 'string'),\n ('skewRG', 'string'),\n ('skewRB', 'string'),\n ('kurtosisRR', 'string'),\n ('kurtosisRG', 'string'),\n ('kurtosisRB', 'string'),\n ('entropyRR', 'string'),\n ('entropyRG', 'string'),\n ('entropyRB', 'string'),\n ('meanH', 'string'),\n ('meanS', 'string'),\n ('meanV', 'string'),\n ('StdDevH', 'string'),\n ('StdDevS', 'string'),\n ('StdDevV', 'string'),\n ('skewH', 'string'),\n ('skewS', 'string'),\n ('skewV', 'string'),\n ('kurtosisH', 'string'),\n ('kurtosisS', 'string'),\n ('kurtosisV', 'string'),\n ('entropyH', 'string'),\n ('entropyS', 'string'),\n ('entropyV', 'string'),\n ('meanL', 'string'),\n ('meanA', 'string'),\n ('meanB', 'string'),\n ('StdDevL', 'string'),\n ('StdDevA', 'string'),\n ('StdDevB', 'string'),\n ('skewL', 'string'),\n ('skewA', 'string'),\n ('skewB', 'string'),\n ('kurtosisL', 'string'),\n ('kurtosisA', 'string'),\n ('kurtosisB', 'string'),\n ('entropyL', 'string'),\n ('entropyA', 'string'),\n ('entropyB', 'string'),\n ('meanY', 'string'),\n ('meanCb', 'string'),\n ('meanCr', 'string'),\n ('StdDevY', 'string'),\n ('StdDevCb', 'string'),\n ('StdDevCr', 'string'),\n ('skewY', 'string'),\n ('skewCb', 'string'),\n ('skewCr', 'string'),\n ('kurtosisY', 'string'),\n ('kurtosisCb', 'string'),\n ('kurtosisCr', 'string'),\n ('entropyY', 'string'),\n ('entropyCb', 'string'),\n ('entropyCr', 'string'),\n ('meanXX', 'string'),\n ('meanYY', 'string'),\n ('meanZZ', 'string'),\n ('StdDevXX', 'string'),\n ('StdDevYY', 'string'),\n ('StdDevZZ', 'string'),\n ('skewXX', 'string'),\n ('skewYY', 'string'),\n ('skewZZ', 'string'),\n ('kurtosisXX', 'string'),\n ('kurtosisYY', 'string'),\n ('kurtosisZZ', 'string'),\n ('entropyXX', 'string'),\n ('entropyYY', 'string'),\n ('entropyZZ', 'string'),\n ('ALLdaub4RR', 'string'),\n ('ALLdaub4RG', 'string'),\n ('ALLdaub4RB', 'string'),\n ('ALLdaub4H', 'string'),\n ('ALLdaub4S', 'string'),\n ('ALLdaub4V', 'string'),\n ('ALLdaub4L', 'string'),\n ('ALLdaub4a', 'string'),\n ('ALLdaub4b', 'string'),\n ('ALLdaub4Y', 'string'),\n ('ALLdaub4Cb', 'string'),\n ('ALLdaub4Cr', 'string'),\n ('ALLdaub4XX', 'string'),\n ('ALLdaub4YY', 'string'),\n ('ALLdaub4ZZ', 'string'),\n ('CLASS', 'string')]"}, "execution_count": 88, "metadata": {}, "output_type": "execute_result"}], "source": "df.dtypes # observamos como se encuentran inicialmente"}, {"cell_type": "markdown", "id": "f5ef9c39-f2c0-4cf3-8da0-a8136dcfb043", "metadata": {}, "source": "#### Vemos que todas las variables tienen que ser reales menos las variables CONVEX_AREA (entero) y CLASS (categ\u00f3rica)"}, {"cell_type": "code", "execution_count": 89, "id": "632e54ae-4d71-4690-83fd-1153de1d5fa8", "metadata": {}, "outputs": [], "source": "# Lista de columnas a convertir\ncols_to_convert = df.columns\ncols_to_convert.remove(\"CONVEX_AREA\")\ncols_to_convert.remove(\"CLASS\")"}, {"cell_type": "code", "execution_count": 91, "id": "1d85b09f-5ec2-4628-aef7-355a41c8023c", "metadata": {}, "outputs": [], "source": "# Crea una copia del DataFrame original\ndf_converted = df\n\n# Itera sobre las columnas y aplica .astype() a cada una\nfor col in cols_to_convert:\n    df_converted = df_converted.withColumn(col, df[col].cast(\"float\"))\n"}, {"cell_type": "code", "execution_count": 92, "id": "bee289cd-67ec-492f-ad22-ecd0ecd03a58", "metadata": {}, "outputs": [], "source": "df_converted = df_converted.withColumn(\"CONVEX_AREA\", df[\"CONVEX_AREA\"].cast(\"integer\"))"}, {"cell_type": "markdown", "id": "f711834b-b0ee-43b5-8611-0ab9273969c7", "metadata": {}, "source": "#### Pasamos cada clase de arroz a entero"}, {"cell_type": "code", "execution_count": 93, "id": "9c003c58-ff73-4fa7-ac37-0558e80240e0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.feature import StringIndexer\nindexer = StringIndexer(inputCol=\"CLASS\", outputCol=\"CLASS_INT\", handleInvalid=\"keep\")\nmodel = indexer.fit(df_converted)\ndf_indexed = model.transform(df_converted)"}, {"cell_type": "markdown", "id": "b0d682fc-7ec0-4db9-90d0-81290f27303c", "metadata": {}, "source": "* 0: Arborio\n* 1: Basmati\n* 2: Karacadag\n* 3: Jasmine\n* 4: Ipsala"}, {"cell_type": "markdown", "id": "8f12be12-aa21-4213-821a-9f3d58c1fde8", "metadata": {}, "source": "#### Extraemos las caracter\u00edsticas de salida"}, {"cell_type": "code", "execution_count": 94, "id": "5b7e5681-4610-4e6f-838a-6deaf4637380", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#df = df_indexed\nlabels = df_indexed.select(\"CLASS\").collect()\nlabels_int = df_indexed.select(\"CLASS_INT\").collect()\ndf_features = df_indexed.drop(\"CLASS\",\"CLASS_INT\")\nfeatures = df_indexed.columns"}, {"cell_type": "code", "execution_count": 95, "id": "75e564a8-680e-4283-8c53-358ccca0cb4d", "metadata": {}, "outputs": [], "source": "selected_columns = [\"ROUNDNESS\", \"COMPACTNESS\", \"SHAPEFACTOR_3\", \"ASPECT_RATIO\", \"ECCENTRICITY\",\"CLASS_INT\"]\nfiltered_df = df_indexed.select(selected_columns)"}, {"cell_type": "code", "execution_count": 96, "id": "4e4fd7c9-8b2d-41d4-826f-d163e4a7f7ee", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import col\nfiltered_df = filtered_df.withColumn(\"CLASS_INT\", col(\"CLASS_INT\").cast(\"integer\"))"}, {"cell_type": "code", "execution_count": 97, "id": "438d19b5-1193-4a49-9536-98c99bb07e57", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+-----------+-------------+------------+------------+---------+\n|ROUNDNESS|COMPACTNESS|SHAPEFACTOR_3|ASPECT_RATIO|ECCENTRICITY|CLASS_INT|\n+---------+-----------+-------------+------------+------------+---------+\n|   0.5114|     0.4751|       0.2257|      4.3693|      0.9735|        1|\n|    0.812|     0.7065|       0.4992|      1.9807|      0.8632|        0|\n|   0.6505|     0.5689|       0.3236|      3.0482|      0.9447|        3|\n|   0.5256|     0.5007|       0.2507|      3.9325|      0.9671|        1|\n|   0.7944|     0.6932|       0.4806|      2.0519|      0.8732|        0|\n|   0.7374|     0.6824|       0.4656|      2.1013|      0.8795|        4|\n|   0.4722|     0.4496|       0.2021|      4.8441|      0.9785|        1|\n|   0.7992|      0.713|       0.5083|      1.9408|       0.857|        0|\n|   0.6673|      0.595|       0.3541|      2.7618|      0.9321|        3|\n|    0.904|     0.7993|       0.6389|      1.5539|      0.7654|        2|\n+---------+-----------+-------------+------------+------------+---------+\nonly showing top 10 rows\n\n"}], "source": "filtered_df.show(10)"}, {"cell_type": "markdown", "id": "da2f9f44-eb99-4263-ae65-0893634296b9", "metadata": {}, "source": "#### Estandarizamos"}, {"cell_type": "code", "execution_count": 98, "id": "57b31d9d-c934-4109-92a7-108014987f45", "metadata": {}, "outputs": [], "source": "featureCols = filtered_df.columns\ndel featureCols[-1]"}, {"cell_type": "code", "execution_count": 99, "id": "6c5d220b-cc88-4c0f-aede-73aa2ffe4cc1", "metadata": {}, "outputs": [], "source": "from pyspark.ml.feature import VectorAssembler, StandardScaler"}, {"cell_type": "code", "execution_count": 100, "id": "daedf3cc-2003-466d-9921-7db0936f6fa6", "metadata": {}, "outputs": [], "source": "# put features into a feature vector column\nassembler = VectorAssembler(inputCols=featureCols, outputCol=\"features_raw\") \nassembled_df = assembler.transform(filtered_df)"}, {"cell_type": "code", "execution_count": 101, "id": "5614d1bc-5f65-419d-9993-5067e38d9337", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+-----------+-------------+------------+------------+---------+--------------------+\n|ROUNDNESS|COMPACTNESS|SHAPEFACTOR_3|ASPECT_RATIO|ECCENTRICITY|CLASS_INT|        features_raw|\n+---------+-----------+-------------+------------+------------+---------+--------------------+\n|   0.5114|     0.4751|       0.2257|      4.3693|      0.9735|        1|[0.51139998435974...|\n|    0.812|     0.7065|       0.4992|      1.9807|      0.8632|        0|[0.81199997663497...|\n|   0.6505|     0.5689|       0.3236|      3.0482|      0.9447|        3|[0.65049999952316...|\n|   0.5256|     0.5007|       0.2507|      3.9325|      0.9671|        1|[0.52560001611709...|\n|   0.7944|     0.6932|       0.4806|      2.0519|      0.8732|        0|[0.79439997673034...|\n|   0.7374|     0.6824|       0.4656|      2.1013|      0.8795|        4|[0.73739999532699...|\n|   0.4722|     0.4496|       0.2021|      4.8441|      0.9785|        1|[0.47220000624656...|\n|   0.7992|      0.713|       0.5083|      1.9408|       0.857|        0|[0.79919999837875...|\n|   0.6673|      0.595|       0.3541|      2.7618|      0.9321|        3|[0.66729998588562...|\n|    0.904|     0.7993|       0.6389|      1.5539|      0.7654|        2|[0.90399998426437...|\n+---------+-----------+-------------+------------+------------+---------+--------------------+\nonly showing top 10 rows\n\n"}], "source": "assembled_df.show(10)"}, {"cell_type": "code", "execution_count": 102, "id": "da185dd6-fa77-44ca-9761-d6ac823b240c", "metadata": {}, "outputs": [], "source": "standardScaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")"}, {"cell_type": "code", "execution_count": 103, "id": "23320ea4-2172-476e-8072-299c53705f51", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "scaled_df = standardScaler.fit(assembled_df).transform(assembled_df)"}, {"cell_type": "code", "execution_count": 104, "id": "ab844636-f75f-412a-80ab-3df99de2e04e", "metadata": {}, "outputs": [], "source": "df_final = scaled_df.select(\"CLASS_INT\",\"features\")"}, {"cell_type": "code", "execution_count": 107, "id": "2b69314c-20f8-47ed-88e0-975d9ed9fded", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+--------------------+\n|CLASS_INT|            features|\n+---------+--------------------+\n|        1|[3.68863331914710...|\n|        0|[5.85680535894486...|\n|        3|[4.69193595175871...|\n|        1|[3.79105551679089...|\n|        0|[5.72985982110623...|\n|        4|[5.31872951796710...|\n|        1|[3.40589114120370...|\n|        0|[5.76448148775941...|\n|        3|[4.81311114016893...|\n|        2|[6.52038436536277...|\n+---------+--------------------+\nonly showing top 10 rows\n\n"}], "source": "df_final.show(10)"}, {"cell_type": "markdown", "id": "11220d28-ff37-41c0-8dfe-fcbc58965c1b", "metadata": {}, "source": "#### Dividimos el conjunto en train y test"}, {"cell_type": "code", "execution_count": 108, "id": "909fcd05-ad50-4f74-9e29-176fc01e0b97", "metadata": {}, "outputs": [], "source": "# Dividir el DataFrame \"df\" en dos conjuntos, uno de entrenamiento (80%) y otro de prueba (20%)\ntrain_data, test_data = df_final.sample(False, 0.8), scaled_df.sample(False, 0.2)"}, {"cell_type": "code", "execution_count": 109, "id": "68b1d009-660d-4106-9866-4874116ef9b3", "metadata": {}, "outputs": [{"data": {"text/plain": "['CLASS_INT', 'features']"}, "execution_count": 109, "metadata": {}, "output_type": "execute_result"}], "source": "train_data.columns"}, {"cell_type": "markdown", "id": "28facf9e-4a3d-480d-9b3c-0457c03215a5", "metadata": {}, "source": "### ENTRENAMOS EL MODELO"}, {"cell_type": "markdown", "id": "df1065bb-328a-4cd1-846a-08d409b2f02a", "metadata": {}, "source": "**M\u00e9trica utilizada:** En problemas de clasificaci\u00f3n con m\u00faltiples clases, una m\u00e9trica com\u00fanmente utilizada es la precisi\u00f3n, que mide cu\u00e1ntas predicciones hechas por el modelo son correctas."}, {"cell_type": "markdown", "id": "f34e1b51-bba3-43cb-a635-c3cc77b6856d", "metadata": {}, "source": "#### **REGRESI\u00d3N LOG\u00cdSTICA**"}, {"cell_type": "code", "execution_count": 110, "id": "48fbf9cb-2c2f-426f-9355-062dcc06c2c2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy:  0.8929240235420011\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 1446:>                                                       (0 + 2) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Precision:  0.8924346485386854\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Importar la clase LogisticRegression\nfrom pyspark.ml.classification import LogisticRegression\n\n# Crear una instancia del modelo\nlr = LogisticRegression(labelCol='CLASS_INT')\n\n# Ajustar el modelo al conjunto de entrenamiento\nmodel_lr = lr.fit(train_data)\n\n# Hacer predicciones en el conjunto de test\npredictions_lr = model_lr.transform(test_data)\n\n# Evaluar el rendimiento del modelo\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nprint('Accuracy: ', MulticlassClassificationEvaluator(labelCol='CLASS_INT', metricName='accuracy').evaluate(predictions_lr))\nprint('Precision: ', MulticlassClassificationEvaluator(labelCol='CLASS_INT', metricName='weightedPrecision').evaluate(predictions_lr))\n"}, {"cell_type": "markdown", "id": "906c0492-c1ba-47fc-8aca-b1162ba3e6ba", "metadata": {}, "source": "**Logramos una precisi\u00f3n de 0.892 con un modelo sencillo, lo que es una buena se\u00f1al.**"}, {"cell_type": "markdown", "id": "cc98eecc-69ca-4789-83a9-eab35756fa2b", "metadata": {}, "source": "#### **DECISION TREE**"}, {"cell_type": "code", "execution_count": 126, "id": "2de55404-2274-49ba-ae08-f293b75a6056", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "maxDepth: 2 Accuracy:  0.7774879614767255 Precision:  0.6692129417473281\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "maxDepth: 3 Accuracy:  0.8832263242375602 Precision:  0.8816088158381881\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "maxDepth: 4 Accuracy:  0.8878410914927769 Precision:  0.8892651864110221\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "maxDepth: 5 Accuracy:  0.889446227929374 Precision:  0.8915924074242241\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "maxDepth: 6 Accuracy:  0.8909176029962547 Precision:  0.8916803872706333\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "maxDepth: 7 Accuracy:  0.8919208132691279 Precision:  0.8926393720497585\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "maxDepth: 8 Accuracy:  0.8924558587479936 Precision:  0.8930025417873692\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "maxDepth: 9 Accuracy:  0.8923889780631353 Precision:  0.8930853433202905\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "maxDepth: 10 Accuracy:  0.89258962011771 Precision:  0.8931902940076617\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Importar la clase DecisionTreeClassifier\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\n# Crear una instancia del modelo\nprecis_dt = 0\nfor i in range(2,11):\n    dt = DecisionTreeClassifier(labelCol='CLASS_INT',maxDepth=i)\n\n    # Ajustar el modelo al conjunto de entrenamiento\n    model_dt = dt.fit(train_data)\n\n    # Hacer predicciones en el conjunto de test\n    predictions_dt = model_dt.transform(test_data)\n\n    # Evaluar el rendimiento del modelo\n    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n    print(\"maxDepth:\",i, 'Accuracy: ', MulticlassClassificationEvaluator(labelCol='CLASS_INT', metricName='accuracy').evaluate(predictions_dt),\n          'Precision: ', MulticlassClassificationEvaluator(labelCol='CLASS_INT', metricName='weightedPrecision').evaluate(predictions_dt))\n    \n    if MulticlassClassificationEvaluator(labelCol='CLASS_INT', metricName='weightedPrecision').evaluate(predictions_dt) > precis_dt:\n        precis = MulticlassClassificationEvaluator(labelCol='CLASS_INT', metricName='weightedPrecision').evaluate(predictions_dt)\n        modelo_dt = model_dt\n"}, {"cell_type": "code", "execution_count": 132, "id": "099ca715-eafe-4cd1-ae81-58223b16b75a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Modelo: DecisionTreeClassificationModel: uid=DecisionTreeClassifier_a277d8f3be6d, depth=10, numNodes=189, numClasses=5, numFeatures=5\nPrecisi\u00f3n: 0.8931902940076617\n"}], "source": "print(\"Modelo:\",modelo_dt)\nprint(\"Precisi\u00f3n:\",precis)"}, {"cell_type": "markdown", "id": "fbfa0dd9-9ea7-46c2-9d6c-1fc48bf759bd", "metadata": {}, "source": "**El modelo con una profundidad de 10 es el que mejor resultado proporciona con una precisi\u00f3n de 0.893.**"}, {"cell_type": "markdown", "id": "1885c21e-0c24-4679-895e-9f00e5af4379", "metadata": {}, "source": "#### **RANDOM FOREST**"}, {"cell_type": "code", "execution_count": 133, "id": "3e7c8d57-4226-4e3a-9f3e-453b8e03e234", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 15 Accuracy:  0.8857677902621723 Precision:  0.8868581889874035\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 16 Accuracy:  0.8893793472445158 Precision:  0.8895714002489461\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 17 Accuracy:  0.8859015516318887 Precision:  0.8868662070594634\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 18 Accuracy:  0.8857677902621723 Precision:  0.8867107074020725\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 19 Accuracy:  0.8860353130016051 Precision:  0.8868852267432172\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 20 Accuracy:  0.8881086142322098 Precision:  0.8885998923230547\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 21 Accuracy:  0.8867710005350454 Precision:  0.8879388809295086\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 22 Accuracy:  0.8859015516318887 Precision:  0.8868327279611345\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 23 Accuracy:  0.8858346709470305 Precision:  0.8867345216670339\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 24 Accuracy:  0.8860353130016051 Precision:  0.8868830120198942\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 25 Accuracy:  0.8857677902621723 Precision:  0.8866525968817732\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 26 Accuracy:  0.8859684323167469 Precision:  0.8869128020971637\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 27 Accuracy:  0.8860353130016051 Precision:  0.8868830120198942\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 28 Accuracy:  0.8859015516318887 Precision:  0.886774788775389\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "numTrees: 29 Accuracy:  0.8856340288924559 Precision:  0.8865960063681685\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Importar la clase RandomForestClassifier\nfrom pyspark.ml.classification import RandomForestClassifier\n\nprecis_rf = 0\n\nfor i in range(15,30):\n    \n    rf = RandomForestClassifier(labelCol='CLASS_INT',numTrees=i)\n\n    # Ajustar el modelo al conjunto de entrenamiento\n    model_rf = rf.fit(train_data)\n\n    # Hacer predicciones en el conjunto de test\n    predictions_rf = model_rf.transform(test_data)\n\n    # Evaluar el rendimiento del modelo\n    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n    print(\"numTrees:\",i, 'Accuracy: ', MulticlassClassificationEvaluator(labelCol='CLASS_INT', metricName='accuracy').evaluate(predictions_rf),\n          'Precision: ', MulticlassClassificationEvaluator(labelCol='CLASS_INT', metricName='weightedPrecision').evaluate(predictions_rf))\n    print()\n    if MulticlassClassificationEvaluator(labelCol='CLASS_INT', metricName='accuracy').evaluate(predictions_rf) > precis_rf:\n        precis_rf = MulticlassClassificationEvaluator(labelCol='CLASS_INT', metricName='weightedPrecision').evaluate(predictions_dt)\n        modelo_rf = model_rf\n"}, {"cell_type": "code", "execution_count": 134, "id": "4780e989-f9cd-498d-9711-cd0a1ae1b5d4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Modelo: RandomForestClassificationModel: uid=RandomForestClassifier_ea4d3e26f22b, numTrees=15, numClasses=5, numFeatures=5\nPrecisi\u00f3n: 0.8931902940076617\n"}], "source": "print(\"Modelo:\",modelo_rf)\nprint(\"Precisi\u00f3n:\",precis_rf)"}, {"cell_type": "markdown", "id": "7a5d6c00-d6e4-4ab2-b68a-8766e0701401", "metadata": {}, "source": "**El modelo de 15 \u00e1rboles consigue una precisi\u00f3n de 0.893**"}, {"cell_type": "markdown", "id": "66677875-08ba-4b90-84ff-c58f83ea51de", "metadata": {}, "source": "## CONCLUSI\u00d3N"}, {"cell_type": "markdown", "id": "7589a91a-4ecb-4890-b6e1-87c48954be6e", "metadata": {}, "source": "**Comparamos la precisi\u00f3n de los modelos**\n\n* Regresi\u00f3n Log\u00edstica: 0.8924\n* Decision Tree: 0.8931\n* Random Forest: 0.8931"}, {"cell_type": "markdown", "id": "e1619783-c8cf-4ffc-aa0e-eca427e9caba", "metadata": {}, "source": "Los tres modelos obtienen unas m\u00e9tricas casi iguales.\n\nEn la pr\u00e1ctica escoger\u00edamos la **regresi\u00f3n l\u00f3gistica** por ser el modelo que m\u00e1s simple que nos proporciona buenos resultados.\n\nAdem\u00e1s podemos tener en cuenta la **exactitud** del modelo, y realizando la comparaci\u00f3n (0.8929 vs. 0.8925 vs. 0.885), se observa como la regresi\u00f3n log\u00edstica sigue siendo la mejor opci\u00f3n."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}